#!/usr/bin/env python3
"""
find_duplicate_pairs.py

Inspect a serialized stereo dataset (produced by create_dataset.py) and report
stereo pairs that are visually identical within optional noise tolerances.

Notebook quick-start:
    # Set the constants below, then run:
    from find_duplicate_pairs import run_duplicate_search
    duplicates = run_duplicate_search()

Command-line usage remains available:
    python src/find_duplicate_pairs.py \
        --bundle ~/path/to/dataset.pt \
        --per_tol 2 \
        --mean_tol 0.5
"""

from __future__ import annotations

import json
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Sequence, Tuple, Union

import torch

# =======================
# === USER SETTINGS ====
# =======================

# Path to the dataset bundle (.pt) generated by create_dataset.py.
BUNDLE_PATH: Union[str, Path, None] = Path(
    "/Users/ryangrgurich/VS Code Local/corner-maze-encoder-pretrain/data/pt-files/all-images-equalized-dataset.pt"
)

# Tolerances for duplicate detection (see _normalize_tolerance for scaling rules).
PER_CHANNEL_TOLERANCE: float = 52.0
MEAN_TOLERANCE: float = 1.0

# Device to run comparisons on (e.g., "cpu", "cuda", "mps").
COMPARISON_DEVICE: Union[str, torch.device] = "cpu"
# Number of candidate samples compared at once when vectorizing differences.
VECTOR_BATCH_SIZE: int = 8192

# Optional path for writing an updated dataset bundle with merged label IDs (None → overwrite input).
OUTPUT_BUNDLE_PATH: Union[str, Path, None] = Path(
    f"/Users/ryangrgurich/VS Code Local/corner-maze-encoder-pretrain/data/pt-files/all-images-equalized-dataset-duplicates-{int(PER_CHANNEL_TOLERANCE)}-{int(MEAN_TOLERANCE)}.pt"
)

# Default path for saving duplicate group report (set to None to disable file output).
REPORT_PATH: Union[str, Path, None] = Path(
    f"/Users/ryangrgurich/VS Code Local/corner-maze-encoder-pretrain/data/reports/all-images-equalized-duplicate-groups-{int(PER_CHANNEL_TOLERANCE)}-{int(MEAN_TOLERANCE)}.json"
)

# Whether to allow overwriting the output bundle/report automatically.
OVERWRITE_OUTPUTS: bool = True


def _normalize_tolerance(value: float) -> float:
    """
    Interpret tolerance in either [0,1] (already normalized) or [0,255].

    Negative values disable the check.
    """
    if value < 0:
        return value
    if value > 1.0:
        return value / 255.0
    return value


def load_dataset(
    bundle_path: Union[str, Path],
    *,
    map_location: Union[str, torch.device] = "cpu",
    device: Union[str, torch.device] = "cpu",
) -> Tuple[torch.Tensor, List[str], List[int], Dict[Union[str, int], Any]]:
    """
    Load the .pt dataset bundle and return the stereo tensor along with label metadata.
    """
    path = Path(bundle_path).expanduser().resolve()
    if not path.exists():
        raise FileNotFoundError(f"Dataset bundle not found: {path}")

    payload = torch.load(path, map_location=map_location)
    if not isinstance(payload, dict):
        raise TypeError(f"Expected dict payload, received {type(payload)!r}")

    if "x" not in payload:
        raise KeyError("Dataset payload missing 'x' tensor")

    # tensor of binocular images with shape (N, 2, H, W)
    tensor = payload["x"]
    if not isinstance(tensor, torch.Tensor):
        raise TypeError(f"Expected 'x' to be torch.Tensor, received {type(tensor)!r}")
    target_device = torch.device(device)
    tensor = tensor.detach().to(target_device)

    label_catalog = payload.get("label_catalog", {})

    def _catalog_lookup(label_id: int, fallback: str) -> str:
        entry = label_catalog.get(label_id) or label_catalog.get(str(label_id))
        if isinstance(entry, dict):
            descriptions = entry.get("descriptions")
            if descriptions:
                return str(descriptions[0])
        return fallback

    label_names_raw = payload.get("label_names")
    if label_names_raw is not None and len(label_names_raw) == tensor.shape[0]:
        label_names = [str(item) for item in label_names_raw]
    else:
        labels_raw = payload.get("labels")
        if labels_raw is None:
            label_names = [f"sample_{idx}" for idx in range(tensor.shape[0])]
        else:
            if isinstance(labels_raw, torch.Tensor):
                iterable = labels_raw.detach().cpu().tolist()
            else:
                iterable = list(labels_raw)
            label_names = []
            for item in iterable:
                try:
                    label_int = int(item)
                except (TypeError, ValueError):
                    label_names.append(str(item))
                    continue
                label_names.append(_catalog_lookup(label_int, f"label_{label_int}"))

    labels_tensor = payload.get("labels")
    if labels_tensor is None:
        label_ids = list(range(tensor.shape[0]))
    elif isinstance(labels_tensor, torch.Tensor):
        label_ids = [int(v) for v in labels_tensor.detach().cpu().tolist()]
    else:
        label_ids = [int(v) for v in labels_tensor]

    if len(label_names) != tensor.shape[0]:
        raise ValueError(
            f"Label count ({len(label_names)}) does not match tensor length ({tensor.shape[0]})."
        )

    if len(label_ids) != tensor.shape[0]:
        raise ValueError(
            f"Label id count ({len(label_ids)}) does not match tensor length ({tensor.shape[0]})."
        )

    return tensor, label_names, label_ids, label_catalog


def group_duplicates(
    stack: torch.Tensor,
    labels: Sequence[str],
    per_channel_tolerance: float,
    mean_tolerance: float,
) -> Tuple[List[List[str]], List[List[int]]]:
    """
    Identify groups of stereo pairs that are equal within tolerance.

    Args:
        stack: Tensor shaped (N, 2, H, W).
        labels: Human-readable identifiers for each tensor in stack.
        per_channel_tolerance: Maximum absolute per-pixel/channel difference allowed.
            Values > 1 are interpreted on a 0-255 scale. Negative → skip check.
        mean_tolerance: Maximum mean absolute difference allowed (same scaling rules).
    """
    if stack.ndim != 4 or stack.shape[1] != 2:
        raise ValueError(f"Expected stack with shape (N, 2, H, W); got {tuple(stack.shape)}")
    if len(labels) != stack.shape[0]:
        raise ValueError("Label list length must equal number of stereo pairs.")

    per_tol = _normalize_tolerance(per_channel_tolerance)
    mean_tol = _normalize_tolerance(mean_tolerance)

    size = stack.shape[0]
    parent = list(range(size))

    def find(x: int) -> int:
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(a: int, b: int) -> None:
        root_a, root_b = find(a), find(b)
        if root_a == root_b:
            return
        parent[root_b] = root_a

    batch_size = max(1, VECTOR_BATCH_SIZE)
    with torch.no_grad():
        for idx_a in range(size - 1):
            sample = stack[idx_a]
            for batch_start in range(idx_a + 1, size, batch_size):
                batch_end = min(size, batch_start + batch_size)
                candidates = stack[batch_start:batch_end]
                if candidates.shape[0] == 0:
                    continue
                diff = torch.abs(candidates - sample)
                diff_flat = diff.view(diff.shape[0], -1)
                per_values = diff_flat.max(dim=1).values
                mean_values = diff_flat.mean(dim=1) if mean_tol >= 0 else None

                if per_tol < 0 and mean_tol < 0:
                    match_mask = per_values == 0.0
                else:
                    match_mask = torch.ones(
                        diff_flat.shape[0], dtype=torch.bool, device=stack.device
                    )
                    if per_tol >= 0:
                        match_mask &= per_values <= per_tol
                    if mean_tol >= 0 and mean_values is not None:
                        match_mask &= mean_values <= mean_tol

                if not match_mask.any().item():
                    continue

                matched_offsets = torch.nonzero(match_mask, as_tuple=False).squeeze(1).tolist()
                for offset in matched_offsets:
                    union(idx_a, batch_start + offset)

    clusters: Dict[int, List[int]] = {}
    for idx in range(size):
        root = find(idx)
        clusters.setdefault(root, []).append(idx)

    duplicate_label_groups: List[List[str]] = []
    duplicate_index_groups: List[List[int]] = []
    for members in clusters.values():
        if len(members) > 1:
            sorted_members = sorted(members)
            duplicate_index_groups.append(sorted_members)
            duplicate_label_groups.append([labels[idx] for idx in sorted_members])

    order = sorted(
        range(len(duplicate_index_groups)),
        key=lambda i: (-len(duplicate_index_groups[i]), duplicate_label_groups[i][0] if duplicate_label_groups[i] else ""),
    )
    duplicate_label_groups = [duplicate_label_groups[i] for i in order]
    duplicate_index_groups = [duplicate_index_groups[i] for i in order]
    return duplicate_label_groups, duplicate_index_groups


def detect_duplicate_groups(
    stack: torch.Tensor,
    labels: Sequence[str],
    per_channel_tolerance: float,
    mean_tolerance: float,
) -> Tuple[List[List[str]], List[List[int]]]:
    """
    Backwards-compatible wrapper for legacy imports.
    """
    return group_duplicates(stack, labels, per_channel_tolerance, mean_tolerance)


def _partition_indices_by_cue(label_names: Sequence[str]) -> Dict[str, List[int]]:
    """
    Separate indices into cued vs non-cued groups based on filename prefix.

    Cued samples: session phase 'trl' with cue token not equal to 'x'.
    All other samples (including ITI) fall into the non_cued bucket.
    """
    partitions: Dict[str, List[int]] = {"cued": [], "non_cued": []}
    for idx, raw_label in enumerate(label_names):
        label = str(raw_label)
        parts = label.split("_")
        group = "non_cued"
        if len(parts) >= 3:
            session_phase = parts[0].lower()
            cue_token = parts[2].lower()
            if session_phase == "trl" and cue_token != "x":
                group = "cued"
        partitions[group].append(idx)
    return partitions


def find_partitioned_duplicate_index_groups(
    stack: torch.Tensor,
    label_names: Sequence[str],
    per_tol: float,
    mean_tol: float,
) -> List[List[int]]:
    """
    Run duplicate detection separately for cued vs non-cued trials and
    return the combined duplicate index groups in terms of the original tensor.
    """
    partitions = _partition_indices_by_cue(label_names)
    all_groups: List[List[int]] = []

    for indices in partitions.values():
        if len(indices) < 2:
            continue
        index_tensor = torch.tensor(indices, dtype=torch.long, device=stack.device)
        subset_stack = stack.index_select(0, index_tensor)
        subset_labels = [label_names[i] for i in indices]
        _, subset_groups = group_duplicates(subset_stack, subset_labels, per_tol, mean_tol)
        for group in subset_groups:
            all_groups.append([indices[i] for i in group])

    return all_groups


def summarize_duplicate_groups(
    duplicate_index_groups: Sequence[Sequence[int]],
    label_ids: Sequence[int],
    label_names: Sequence[str],
    label_catalog: Dict[Union[str, int], Any],
) -> List[Dict[str, Any]]:
    """
    Build a summary for each duplicate group including shared label ID and descriptions.
    """
    summaries: List[Dict[str, Any]] = []
    for indices in duplicate_index_groups:
        if not indices:
            continue
        ids_in_group = {int(label_ids[idx]) for idx in indices}
        canonical_id = min(ids_in_group)
        descriptions = {str(label_names[idx]) for idx in indices}
        for label_id in ids_in_group:
            entry = label_catalog.get(label_id) or label_catalog.get(str(label_id))
            if isinstance(entry, dict):
                desc_list = entry.get("descriptions") or []
            elif isinstance(entry, (list, tuple, set)):
                desc_list = entry
            else:
                desc_list = []
            for desc in desc_list:
                descriptions.add(str(desc))
        summaries.append(
            {
                "label_id": int(canonical_id),
                "descriptions": sorted(descriptions),
                "size": len(indices),
            }
        )

    summaries.sort(key=lambda item: (-item["size"], item["label_id"]))
    return summaries


def apply_duplicate_label_updates(
    bundle_path: Union[str, Path],
    duplicate_index_groups: Sequence[Sequence[int]],
    *,
    label_names_fallback: Sequence[str] | None = None,
    output_path: Union[str, Path, None] = None,
    overwrite: bool = False,
) -> Dict[str, Union[str, int, bool]]:
    """
    Update label IDs so duplicate samples share the same class ID, rebuild the catalog, and save the bundle.
    """
    if not duplicate_index_groups:
        return {
            "output_path": str(Path(bundle_path).expanduser().resolve()),
            "updated": False,
            "groups": 0,
        }

    src_path = Path(bundle_path).expanduser().resolve()
    payload = torch.load(src_path, map_location="cpu")
    if "x" not in payload:
        raise KeyError("Dataset payload missing 'x' tensor")

    stack = payload["x"]
    if not isinstance(stack, torch.Tensor):
        raise TypeError(f"Expected 'x' to be torch.Tensor, received {type(stack)!r}")

    size = stack.shape[0]

    label_names = payload.get("label_names")
    if label_names is None or len(label_names) != size:
        if label_names_fallback is not None and len(label_names_fallback) == size:
            label_names = list(map(str, label_names_fallback))
        else:
            raise KeyError("Dataset payload missing 'label_names' aligned with samples.")
    else:
        label_names = [str(item) for item in label_names]

    raw_labels = payload.get("labels")
    if raw_labels is None:
        label_ids = list(range(size))
    elif isinstance(raw_labels, torch.Tensor):
        label_ids = [int(v) for v in raw_labels.detach().cpu().tolist()]
    else:
        label_ids = [int(v) for v in raw_labels]

    if len(label_ids) != size:
        raise ValueError(f"Label list length ({len(label_ids)}) does not match tensor length ({size}).")

    updated = False
    for group in duplicate_index_groups:
        if not group:
            continue
        canonical_id = min(label_ids[idx] for idx in group)
        for idx in group:
            if label_ids[idx] != canonical_id:
                label_ids[idx] = canonical_id
                updated = True

    catalog_builder: Dict[int, set] = defaultdict(set)
    catalog_meta: Dict[int, Dict[str, Any]] = defaultdict(dict)

    def _merge_metadata(target: Dict[str, Any], source: Dict[str, Any]) -> None:
        for key, value in source.items():
            if key == "descriptions":
                continue
            target.setdefault(key, value)

    def _ingest_catalog(source: Dict[Union[str, int], Any]) -> None:
        for key, value in source.items():
            try:
                label_id = int(key)
            except (TypeError, ValueError):
                continue
            descriptions = []
            if isinstance(value, dict):
                descriptions = value.get("descriptions") or []
                _merge_metadata(catalog_meta[label_id], value)
            if isinstance(descriptions, (list, tuple, set)):
                for desc in descriptions:
                    catalog_builder[label_id].add(str(desc))

    existing_catalog = payload.get("label_catalog", {})
    if isinstance(existing_catalog, dict):
        _ingest_catalog(existing_catalog)

    for label_id, description in zip(label_ids, label_names):
        catalog_builder[int(label_id)].add(str(description))

    label_catalog = {
        label_id: {
            **catalog_meta.get(label_id, {}),
            "descriptions": sorted(descriptions),
        }
        for label_id, descriptions in catalog_builder.items()
    }
    label2idx = {
        description: label_id
        for label_id, info in label_catalog.items()
        for description in info["descriptions"]
    }
    idx2label = {
        label_id: info["descriptions"][0]
        for label_id, info in label_catalog.items()
    }

    y_tensor = torch.tensor(label_ids, dtype=torch.long, device=stack.device)

    payload["labels"] = label_ids
    payload["label_catalog"] = label_catalog
    payload["label2idx"] = label2idx
    payload["idx2label"] = idx2label
    payload["y"] = y_tensor
    payload.setdefault("label_names", label_names)

    meta = dict(payload.get("meta", {}))
    meta.update(
        {
            "count": size,
            "label_dtype": str(y_tensor.dtype),
            "catalog_size": len(label_catalog),
            "label_updates": {
                "groups_processed": len(duplicate_index_groups),
                "labels_changed": int(updated),
            },
        }
    )
    payload["meta"] = meta

    dest_path = Path(output_path).expanduser().resolve() if output_path not in (None, "None", "none") else src_path

    if dest_path.exists() and not overwrite:
        return {
            "output_path": str(dest_path),
            "updated": False,
            "groups": len(duplicate_index_groups),
            "skipped_write": True,
        }

    dest_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(payload, dest_path)

    return {
        "output_path": str(dest_path),
        "updated": updated,
        "groups": len(duplicate_index_groups),
        "skipped_write": False,
    }


def run_duplicate_search(
    bundle: Union[str, Path, None] = None,
    per_tol: float | None = None,
    mean_tol: float | None = None,
) -> List[Dict[str, Any]]:
    """
    Notebook-friendly entry point. Falls back to module-level constants when parameters are omitted.
    Returns a list of summaries where each item contains a canonical label_id, group size, and descriptions.
    """
    if bundle is None:
        if BUNDLE_PATH is None:
            raise ValueError("Bundle path not provided. Set BUNDLE_PATH or pass bundle=...")
        bundle = BUNDLE_PATH

    per_tol = PER_CHANNEL_TOLERANCE if per_tol is None else per_tol
    mean_tol = MEAN_TOLERANCE if mean_tol is None else mean_tol
    stack, label_names, label_ids, label_catalog = load_dataset(
        bundle, device=COMPARISON_DEVICE
    )
    index_groups = find_partitioned_duplicate_index_groups(stack, label_names, per_tol, mean_tol)
    return summarize_duplicate_groups(index_groups, label_ids, label_names, label_catalog)


def process_duplicates(
    bundle_path: Union[str, Path, None] = BUNDLE_PATH,
    per_tol: float = PER_CHANNEL_TOLERANCE,
    mean_tol: float = MEAN_TOLERANCE,
    output_bundle: Union[str, Path, None] = OUTPUT_BUNDLE_PATH,
    report_path: Union[str, Path, None] = REPORT_PATH,
    overwrite: bool = OVERWRITE_OUTPUTS,
) -> Dict[str, Any]:
    """
    High-level helper: detect duplicates, optionally save a report, and update the dataset bundle.
    Returns a dictionary containing summary info and the update result.
    """
    if bundle_path is None:
        raise ValueError("Bundle path is required. Set BUNDLE_PATH or pass bundle_path=...")

    stack, label_names, label_ids, label_catalog = load_dataset(
        bundle_path, device=COMPARISON_DEVICE
    )
    index_groups = find_partitioned_duplicate_index_groups(stack, label_names, per_tol, mean_tol)
    summaries = summarize_duplicate_groups(index_groups, label_ids, label_names, label_catalog)

    report_arg = report_path
    if report_arg not in (None, "None", "none"):
        report_path = Path(report_arg).expanduser().resolve()
        if report_path.exists() and not overwrite:
            report_saved = str(report_path)
            report_skipped = True
        else:
            report_path.parent.mkdir(parents=True, exist_ok=True)
            with report_path.open("w", encoding="utf-8") as fh:
                json.dump({"duplicate_groups": summaries}, fh, indent=2)
            report_saved = str(report_path)
            report_skipped = False
    else:
        report_saved = None
        report_skipped = False

    update_result = apply_duplicate_label_updates(
        bundle_path,
        index_groups,
        label_names_fallback=label_names,
        output_path=output_bundle,
        overwrite=overwrite,
    )

    return {
        "summaries": summaries,
        "report_path": report_saved,
        "report_skipped": report_skipped,
        "update": update_result,
    }


if __name__ == "__main__":
    result = process_duplicates()
    summaries = result["summaries"]
    if not summaries:
        print("No duplicate groups found with the configured tolerances.")
    else:
        print(f"Detected {len(summaries)} duplicate group(s):")
        for summary in summaries:
            print(f"  Label {summary['label_id']} (size {summary['size']}):")
            for desc in summary["descriptions"]:
                print(f"    - {desc}")

    if result["report_path"]:
        if result.get("report_skipped"):
            print(f"Duplicate report already exists at {result['report_path']}; skipping write.")
        else:
            print(f"Duplicate report saved to {result['report_path']}.")

    update = result["update"]
    if update.get("skipped_write"):
        print(
            f"Dataset bundle already exists at {update['output_path']}; skipping write (set OVERWRITE_OUTPUTS=True to overwrite)."
        )
    elif update["updated"]:
        print(
            f"Updated label IDs written to {update['output_path']} "
            f"({update['groups']} groups processed)."
        )
    else:
        print(
            f"No label ID changes were necessary. "
            f"Existing bundle remains at {update['output_path']}."
        )
