#!/usr/bin/env python3
"""
find_duplicate_pairs.py

Inspect a serialized stereo dataset (produced by create_dataset.py) and report
stereo pairs that are visually identical within optional noise tolerances.

Notebook quick-start:
    # Set the constants below, then run:
    from find_duplicate_pairs import run_duplicate_search
    duplicates = run_duplicate_search()

Command-line usage remains available:
    python src/find_duplicate_pairs.py \
        --bundle ~/path/to/dataset.pt \
        --per_tol 2 \
        --mean_tol 0.5
"""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Dict, List, Sequence, Tuple, Union

import torch

# =======================
# === USER SETTINGS ====
# =======================

# Path to the dataset bundle (.pt) generated by create_dataset.py.
# Update this to point at your dataset before running in a notebook.
BUNDLE_PATH: Union[str, Path, None] = Path(
    "/Users/ryangrgurich/VS Code Local/corner-maze-encoder-pretrain/data/pt-files/all-views-dataset.pt"
)

# Tolerances for duplicate detection (see _normalize_tolerance for scaling rules).
PER_CHANNEL_TOLERANCE: float = 52
MEAN_TOLERANCE: float = 1

# Include singleton entries when using run_duplicate_search() without overrides.
SHOW_SINGLETONS: bool = False


def _normalize_tolerance(value: float) -> float:
    """
    Interpret tolerance in either [0,1] (already normalized) or [0,255].

    Negative values disable the check.
    """
    if value < 0:
        return value
    if value > 1.0:
        return value / 255.0
    return value


def load_dataset(bundle_path: Union[str, Path], map_location: str = "cpu") -> Tuple[torch.Tensor, Sequence[str]]:
    """
    Load the .pt dataset bundle and return the stereo tensor plus label strings.
    """
    path = Path(bundle_path).expanduser().resolve()
    if not path.exists():
        raise FileNotFoundError(f"Dataset bundle not found: {path}")

    payload = torch.load(path, map_location=map_location)
    if not isinstance(payload, dict):
        raise TypeError(f"Expected dict payload, received {type(payload)!r}")

    if "x" not in payload:
        raise KeyError("Dataset payload missing 'x' tensor")

    tensor = payload["x"]
    if not isinstance(tensor, torch.Tensor):
        raise TypeError(f"Expected 'x' to be torch.Tensor, received {type(tensor)!r}")

    labels = payload.get("labels")
    if labels is None:
        labels = [f"sample_{idx}" for idx in range(tensor.shape[0])]

    if len(labels) != tensor.shape[0]:
        raise ValueError(
            f"Label count ({len(labels)}) does not match tensor length ({tensor.shape[0]})."
        )

    return tensor.detach().cpu(), list(map(str, labels))


def group_duplicates(
    stack: torch.Tensor,
    labels: Sequence[str],
    per_channel_tolerance: float,
    mean_tolerance: float,
) -> List[List[str]]:
    """
    Identify groups of stereo pairs that are equal within tolerance.

    Args:
        stack: Tensor shaped (N, 2, H, W).
        labels: Human-readable identifiers for each tensor in stack.
        per_channel_tolerance: Maximum absolute per-pixel/channel difference allowed.
            Values > 1 are interpreted on a 0-255 scale. Negative â†’ skip check.
        mean_tolerance: Maximum mean absolute difference allowed (same scaling rules).
    """
    if stack.ndim != 4 or stack.shape[1] != 2:
        raise ValueError(f"Expected stack with shape (N, 2, H, W); got {tuple(stack.shape)}")
    if len(labels) != stack.shape[0]:
        raise ValueError("Label list length must equal number of stereo pairs.")

    per_tol = _normalize_tolerance(per_channel_tolerance)
    mean_tol = _normalize_tolerance(mean_tolerance)

    def _pairs_similar(idx_a: int, idx_b: int) -> bool:
        diff = torch.abs(stack[idx_a] - stack[idx_b])
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()

        if per_tol < 0 and mean_tol < 0:
            return max_diff == 0.0

        if per_tol >= 0 and max_diff > per_tol:
            return False
        if mean_tol >= 0 and mean_diff > mean_tol:
            return False
        return True

    size = stack.shape[0]
    parent = list(range(size))

    def find(x: int) -> int:
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(a: int, b: int) -> None:
        root_a, root_b = find(a), find(b)
        if root_a == root_b:
            return
        parent[root_b] = root_a

    for i in range(size):
        for j in range(i + 1, size):
            if _pairs_similar(i, j):
                union(i, j)

    clusters: Dict[int, List[int]] = {}
    for idx in range(size):
        root = find(idx)
        clusters.setdefault(root, []).append(idx)

    duplicate_groups: List[List[str]] = []
    for members in clusters.values():
        if len(members) > 1:
            duplicate_groups.append(sorted(labels[idx] for idx in members))

    duplicate_groups.sort(key=lambda items: (-len(items), items[0]))
    return duplicate_groups


def parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Find duplicate stereo image pairs in a dataset bundle."
    )
    parser.add_argument(
        "--bundle",
        default=BUNDLE_PATH,
        help="Path to the dataset .pt file (defaults to BUNDLE_PATH constant when omitted).",
    )
    parser.add_argument(
        "--per_tol",
        type=float,
        default=PER_CHANNEL_TOLERANCE,
        help="Per-pixel tolerance (0-1 range, or 0-255 if >1). Negative disables the per-pixel check.",
    )
    parser.add_argument(
        "--mean_tol",
        type=float,
        default=MEAN_TOLERANCE,
        help="Mean absolute difference tolerance (same scaling rules as per_tol). Negative disables the check.",
    )
    parser.add_argument(
        "--show-singletons",
        action="store_true",
        default=SHOW_SINGLETONS,
        help="Include all entries (even unique ones) in the output.",
    )
    return parser.parse_args(argv)


def run_duplicate_search(
    bundle: Union[str, Path, None] = None,
    per_tol: float | None = None,
    mean_tol: float | None = None,
    show_singletons: bool | None = None,
) -> List[List[str]]:
    """
    Notebook-friendly entry point. Falls back to module-level constants when parameters are omitted.
    Returns a list of duplicate groups (each group is a list of label strings).
    """
    if bundle is None:
        if BUNDLE_PATH is None:
            raise ValueError("Bundle path not provided. Set BUNDLE_PATH or pass bundle=...")
        bundle = BUNDLE_PATH

    per_tol = PER_CHANNEL_TOLERANCE if per_tol is None else per_tol
    mean_tol = MEAN_TOLERANCE if mean_tol is None else mean_tol
    show_singletons = SHOW_SINGLETONS if show_singletons is None else show_singletons

    stack, labels = load_dataset(bundle)
    groups = group_duplicates(stack, labels, per_tol, mean_tol)

    if show_singletons:
        if groups:
            covered = {label for group in groups for label in group}
        else:
            covered = set()
        for label in labels:
            if label not in covered:
                groups.append([label])
        groups.sort(key=lambda items: (-len(items), items[0]))

    return groups


def main(argv: Sequence[str] | None = None) -> None:
    args = parse_args(argv)
    if args.bundle is None:
        raise ValueError(
            "Bundle path is required. Set BUNDLE_PATH or supply --bundle on the command line."
        )

    groups = run_duplicate_search(
        bundle=args.bundle,
        per_tol=args.per_tol,
        mean_tol=args.mean_tol,
        show_singletons=args.show_singletons,
    )

    if not groups:
        print("No duplicate groups found with the provided tolerances.")
        return

    print(f"Detected {len(groups)} duplicate group(s):")
    for idx, group in enumerate(groups, 1):
        print(f"  Group {idx} (size {len(group)}):")
        for label in group:
            print(f"    - {label}")


if __name__ == "__main__":
    main()
