#!/usr/bin/env python3
"""
group_duplicates.py

Utility functions for identifying visually duplicate stereo pairs within the dataset
bundle produced by `create_dataset.py`.
Use visualize_image_stack after this script to inspect groupings. Modify the csv file to fix groupings
as you see fit. I found that a PER_CHANNEL_TOLERANCE of 52 (on a 0-255 scale) and a MEAN_TOLERANCE of 1
did an excellent job of grouping similar images while allowing for render noise.
"""

from __future__ import annotations

from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Sequence, Tuple, Union

import torch
import dataset_io

# =======================
# === USER SETTINGS ====
# =======================

# Path to the dataset directory generated by create_dataset.py (dataset_io bundle).
ROOT = Path(__file__).resolve().parents[1]
BUNDLE_PATH = ROOT / "data/datasets/base-images-ds"

# Tolerances for duplicate detection (see _normalize_tolerance for scaling rules).
PER_CHANNEL_TOLERANCE: float = 52.0
MEAN_TOLERANCE: float = 1.0

# Optional path for writing an updated dataset bundle with merged label IDs (None → overwrite input).
OUTPUT_BUNDLE_PATH = BUNDLE_PATH.parent / f"{BUNDLE_PATH.name.removesuffix('-ds')}-groups-partitioned-{int(PER_CHANNEL_TOLERANCE)}-{int(MEAN_TOLERANCE)}-ds"

# Device to run comparisons on (e.g., "cpu", "cuda", "mps").
COMPARISON_DEVICE: Union[str, torch.device] = "cpu"

# Number of candidate samples compared at once when vectorizing differences.
VECTOR_BATCH_SIZE: int = 500
CSV_OUTPUT_PATH: Union[str, Path, None] = None
# Optional explicit CSV file name (without directory). Overrides auto-generated name when set.
CSV_FILE_NAME = f"{OUTPUT_BUNDLE_PATH.name.removesuffix('-ds')}.csv"

# Whether to allow overwriting the output bundle/report automatically.
OVERWRITE_OUTPUTS: bool = True

# Toggle whether duplicate detection should partition by cued vs non_cued configurations.
PARTITION_BY_CUE: bool = True


def _normalize_tolerance(value: float) -> float:
    """
    Interpret tolerance in either [0,1] (already normalized) or [0,255].

    Negative values disable the check.
    """
    if value < 0:
        return value
    if value > 1.0:
        return value / 255.0
    return value


def _singleton_indices(
    total_count: int,
    duplicate_groups: Sequence[Sequence[int]],
) -> List[int]:
    duplicate_members = {idx for group in duplicate_groups for idx in group}
    return [idx for idx in range(total_count) if idx not in duplicate_members]


def _build_singleton_summaries(
    singleton_indices: Sequence[int],
    label_ids: Sequence[int],
    label_names: Sequence[str],
    labels2label_names: Dict[int, List[str]],
) -> List[Dict[str, Any]]:
    summaries: List[Dict[str, Any]] = []

    for idx in singleton_indices:
        label_id = int(label_ids[idx])
        descriptions = {str(label_names[idx])}
        for desc in labels2label_names.get(label_id, []):
            descriptions.add(str(desc))
        summaries.append(
            {
                "label_id": label_id,
                "descriptions": sorted(descriptions),
                "size": 1,
            }
        )

    summaries.sort(key=lambda item: item["descriptions"][0] if item["descriptions"] else "")
    return summaries


def _format_tolerance_tag(value: float | None) -> str:
    if value is None:
        return "auto"
    try:
        numeric = float(value)
    except (TypeError, ValueError):
        return str(value)
    if numeric.is_integer():
        return str(int(numeric))
    return str(numeric).replace(".", "_")


def _default_csv_path(bundle_dir: Path, per_tol: float | None, mean_tol: float | None) -> Path:
    bundle_dir = _csv_parent_for_target(bundle_dir)
    if CSV_FILE_NAME:
        return bundle_dir / CSV_FILE_NAME
    prefix = bundle_dir.name.removesuffix("-ds") or bundle_dir.name
    per_tag = _format_tolerance_tag(per_tol)
    mean_tag = _format_tolerance_tag(mean_tol)
    return bundle_dir / f"{prefix}-duplicate-groups-{per_tag}-{mean_tag}.csv"


def _write_group_csv(
    csv_path: Path,
    ordered_groups: Sequence[Sequence[str]],
    overwrite: bool,
) -> Tuple[str, bool]:
    if csv_path.exists() and not overwrite:
        return str(csv_path), True
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    with csv_path.open("w", encoding="utf-8") as fh:
        for group in ordered_groups:
            fh.write(",".join(map(str, group)) + "\n")
    return str(csv_path), False


def _catalog_label_order(
    labels2label_names: Dict[int, List[str]],
    label_ids: Sequence[int],
) -> Dict[int, int]:
    order: Dict[int, int] = {}
    for position, label_id in enumerate(labels2label_names.keys()):
        try:
            order[int(label_id)] = position
        except (TypeError, ValueError):
            continue
    next_position = len(order)
    for raw_id in label_ids:
        try:
            label_id = int(raw_id)
        except (TypeError, ValueError):
            continue
        if label_id not in order:
            order[label_id] = next_position
            next_position += 1
    return order


def _ordered_groups_by_catalog(
    duplicate_label_groups: Sequence[Sequence[str]],
    duplicate_index_groups: Sequence[Sequence[int]],
    singleton_groups: Sequence[Sequence[str]],
    singleton_indices: Sequence[int],
    label_ids: Sequence[int],
    labels2label_names: Dict[int, List[str]],
) -> List[Sequence[str]]:
    """
    Combine duplicate and singleton groups into a single ordered list that follows
    the label ordering established by labels2label_names.
    """
    order_lookup = _catalog_label_order(labels2label_names, label_ids)
    fallback_order = len(order_lookup)

    def _canonical_id(indices: Sequence[int]) -> int | None:
        if not indices:
            return None
        resolved = [label_ids[idx] for idx in indices]
        try:
            return int(min(resolved))
        except (TypeError, ValueError):
            return None

    entries: List[Tuple[int, int, Sequence[str]]] = []
    for group_labels, index_group in zip(duplicate_label_groups, duplicate_index_groups):
        canonical = _canonical_id(index_group)
        order_value = order_lookup.get(int(canonical), fallback_order) if canonical is not None else fallback_order
        entries.append((order_value, 0, group_labels))

    for idx, group_labels in zip(singleton_indices, singleton_groups):
        try:
            label_id = int(label_ids[idx])
        except (TypeError, ValueError):
            label_id = None
        order_value = order_lookup.get(label_id, fallback_order)
        entries.append((order_value, 1, group_labels))

    entries.sort(key=lambda item: (item[0], item[1], item[2][0] if item[2] else ""))
    return [item[2] for item in entries]


def _csv_parent_for_target(path: Path) -> Path:
    """
    If the path points to a directory (or directory-like), return it.
    Otherwise, return the parent directory (supports .pt or other file outputs).
    """
    if path.suffix:
        return path.parent
    return path


def load_dataset(
    bundle_path: Union[str, Path],
    *,
    device: Union[str, torch.device] = "cpu",
) -> Tuple[torch.Tensor, List[str], List[int], Dict[int, List[str]]]:
    """
    Load the dataset bundle and return stereo tensors plus metadata.
    """
    payload = dataset_io.load_bundle(Path(bundle_path))

    if "x" not in payload:
        raise KeyError("Dataset payload missing 'x' tensor")

    # tensor of binocular images with shape (N, 2, H, W)
    tensor = payload["x"]
    if not isinstance(tensor, torch.Tensor):
        raise TypeError(f"Expected 'x' to be torch.Tensor, received {type(tensor)!r}")
    target_device = torch.device(device)
    tensor = tensor.detach().to(target_device)

    labels_map_raw = payload.get("labels2label_names", {})
    labels2label_names: Dict[int, List[str]] = {}
    if isinstance(labels_map_raw, dict):
        for key, value in labels_map_raw.items():
            try:
                label_id = int(key)
            except (TypeError, ValueError):
                continue
            if isinstance(value, (list, tuple, set)):
                labels = [str(v) for v in value]
            elif value is None:
                labels = []
            else:
                labels = [str(value)]
            labels2label_names[label_id] = sorted(set(labels))

    def _catalog_lookup(label_id: int, fallback: str) -> str:
        descriptions = labels2label_names.get(label_id)
        if descriptions:
            return str(descriptions[0])
        return fallback

    label_names_raw = payload.get("label_names")
    if label_names_raw is not None and len(label_names_raw) == tensor.shape[0]:
        label_names = [str(item) for item in label_names_raw]
    else:
        labels_raw = payload.get("labels")
        if labels_raw is None:
            label_names = [f"sample_{idx}" for idx in range(tensor.shape[0])]
        else:
            if isinstance(labels_raw, torch.Tensor):
                iterable = labels_raw.detach().cpu().tolist()
            else:
                iterable = list(labels_raw)
            label_names = []
            for item in iterable:
                try:
                    label_int = int(item)
                except (TypeError, ValueError):
                    label_names.append(str(item))
                    continue
                label_names.append(_catalog_lookup(label_int, f"label_{label_int}"))

    labels_tensor = payload.get("labels")
    if labels_tensor is None:
        label_ids = list(range(tensor.shape[0]))
    elif isinstance(labels_tensor, torch.Tensor):
        label_ids = [int(v) for v in labels_tensor.detach().cpu().tolist()]
    else:
        label_ids = [int(v) for v in labels_tensor]

    if len(label_names) != tensor.shape[0]:
        raise ValueError(
            f"Label count ({len(label_names)}) does not match tensor length ({tensor.shape[0]})."
        )

    if len(label_ids) != tensor.shape[0]:
        raise ValueError(
            f"Label id count ({len(label_ids)}) does not match tensor length ({tensor.shape[0]})."
        )

    return tensor, label_names, label_ids, labels2label_names


def group_duplicates(
    stack: torch.Tensor,
    labels: Sequence[str],
    per_channel_tolerance: float,
    mean_tolerance: float,
) -> Tuple[List[List[str]], List[List[int]]]:
    """
    Identify groups of stereo pairs that are equal within tolerance.

    Args:
        stack: Tensor shaped (N, 2, H, W).
        labels: Human-readable identifiers for each tensor in stack.
        per_channel_tolerance: Maximum absolute per-pixel/channel difference allowed.
            Values > 1 are interpreted on a 0-255 scale. Negative → skip check.
        mean_tolerance: Maximum mean absolute difference allowed (same scaling rules).
    """
    if stack.ndim != 4 or stack.shape[1] != 2:
        raise ValueError(f"Expected stack with shape (N, 2, H, W); got {tuple(stack.shape)}")
    if len(labels) != stack.shape[0]:
        raise ValueError("Label list length must equal number of stereo pairs.")

    per_tol = _normalize_tolerance(per_channel_tolerance)
    mean_tol = _normalize_tolerance(mean_tolerance)

    size = stack.shape[0]
    parent = list(range(size))

    def find(x: int) -> int:
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(a: int, b: int) -> None:
        root_a, root_b = find(a), find(b)
        if root_a == root_b:
            return
        parent[root_b] = root_a

    batch_size = max(1, VECTOR_BATCH_SIZE)
    with torch.no_grad():
        for idx_a in range(size - 1):
            sample = stack[idx_a]
            for batch_start in range(idx_a + 1, size, batch_size):
                batch_end = min(size, batch_start + batch_size)
                candidates = stack[batch_start:batch_end]
                if candidates.shape[0] == 0:
                    continue
                diff = torch.abs(candidates - sample)
                diff_flat = diff.view(diff.shape[0], -1)
                per_values = diff_flat.max(dim=1).values
                mean_values = diff_flat.mean(dim=1) if mean_tol >= 0 else None

                if per_tol < 0 and mean_tol < 0:
                    match_mask = per_values == 0.0
                else:
                    match_mask = torch.ones(
                        diff_flat.shape[0], dtype=torch.bool, device=stack.device
                    )
                    if per_tol >= 0:
                        match_mask &= per_values <= per_tol
                    if mean_tol >= 0 and mean_values is not None:
                        match_mask &= mean_values <= mean_tol

                if not match_mask.any().item():
                    continue

                matched_offsets = torch.nonzero(match_mask, as_tuple=False).squeeze(1).tolist()
                for offset in matched_offsets:
                    union(idx_a, batch_start + offset)

    clusters: Dict[int, List[int]] = {}
    for idx in range(size):
        root = find(idx)
        clusters.setdefault(root, []).append(idx)

    duplicate_label_groups: List[List[str]] = []
    duplicate_index_groups: List[List[int]] = []
    for members in clusters.values():
        if len(members) > 1:
            sorted_members = sorted(members)
            duplicate_index_groups.append(sorted_members)
            duplicate_label_groups.append([labels[idx] for idx in sorted_members])

    order = sorted(
        range(len(duplicate_index_groups)),
        key=lambda i: (-len(duplicate_index_groups[i]), duplicate_label_groups[i][0] if duplicate_label_groups[i] else ""),
    )
    duplicate_label_groups = [duplicate_label_groups[i] for i in order]
    duplicate_index_groups = [duplicate_index_groups[i] for i in order]
    return duplicate_label_groups, duplicate_index_groups


def detect_duplicate_groups(
    stack: torch.Tensor,
    labels: Sequence[str],
    per_channel_tolerance: float,
    mean_tolerance: float,
) -> Tuple[List[List[str]], List[List[int]]]:
    """
    Backwards-compatible wrapper for legacy imports.
    """
    return group_duplicates(stack, labels, per_channel_tolerance, mean_tolerance)


def _partition_indices_by_cue(label_names: Sequence[str]) -> Dict[str, List[int]]:
    """
    Separate indices into cued vs non-cued groups based on filename prefix.

    Cued samples: session phase 'trl' with cue token not equal to 'x'.
    All other samples (including ITI) fall into the non_cued bucket.
    """
    partitions: Dict[str, List[int]] = {"cued": [], "non_cued": []}
    for idx, raw_label in enumerate(label_names):
        label = str(raw_label)
        parts = label.split("_")
        group = "non_cued"
        if len(parts) >= 3:
            session_phase = parts[0].lower()
            cue_token = parts[2].lower()
            if session_phase == "trl" and cue_token != "x":
                group = "cued"
        partitions[group].append(idx)
    return partitions


def find_partitioned_duplicate_index_groups(
    stack: torch.Tensor,
    label_names: Sequence[str],
    per_tol: float,
    mean_tol: float,
    partition: bool = True,
) -> List[List[int]]:
    """
    Run duplicate detection with optional partitioning for cued vs non-cued trials.
    When `partition` is True (default), groups never mix cue categories.
    """
    if not partition:
        _, groups = group_duplicates(stack, label_names, per_tol, mean_tol)
        return groups

    partitions = _partition_indices_by_cue(label_names)
    all_groups: List[List[int]] = []

    for indices in partitions.values():
        if len(indices) < 2:
            continue
        index_tensor = torch.tensor(indices, dtype=torch.long, device=stack.device)
        subset_stack = stack.index_select(0, index_tensor)
        subset_labels = [label_names[i] for i in indices]
        _, subset_groups = group_duplicates(subset_stack, subset_labels, per_tol, mean_tol)
        for group in subset_groups:
            all_groups.append([indices[i] for i in group])

    return all_groups


def summarize_duplicate_groups(
    duplicate_index_groups: Sequence[Sequence[int]],
    label_ids: Sequence[int],
    label_names: Sequence[str],
    labels2label_names: Dict[int, List[str]],
) -> List[Dict[str, Any]]:
    """
    Build a summary for each duplicate group including shared label ID and descriptions.
    """
    summaries: List[Dict[str, Any]] = []
    for indices in duplicate_index_groups:
        if not indices:
            continue
        ids_in_group = {int(label_ids[idx]) for idx in indices}
        canonical_id = min(ids_in_group)
        descriptions = {str(label_names[idx]) for idx in indices}
        for label_id in ids_in_group:
            for desc in labels2label_names.get(label_id, []):
                descriptions.add(str(desc))
        summaries.append(
            {
                "label_id": int(canonical_id),
                "descriptions": sorted(descriptions),
                "size": len(indices),
            }
        )

    summaries.sort(key=lambda item: (-item["size"], item["label_id"]))
    return summaries


def apply_duplicate_label_updates(
    bundle_path: Union[str, Path],
    duplicate_index_groups: Sequence[Sequence[int]],
    *,
    label_names_fallback: Sequence[str] | None = None,
    output_path: Union[str, Path, None] = None,
    overwrite: bool = False,
) -> Dict[str, Union[str, int, bool]]:
    """
    Update label IDs so duplicate samples share the same class ID, rebuild the catalog, and save the bundle.
    """
    if not duplicate_index_groups:
        return {
            "output_path": str(Path(bundle_path).expanduser().resolve()),
            "updated": False,
            "groups": 0,
        }

    src_path = Path(bundle_path).expanduser().resolve()
    if src_path.is_file():
        raise ValueError(
            "apply_duplicate_label_updates expects a dataset directory (containing metadata.json). "
            f"Received file path: {src_path}"
        )
    payload = dataset_io.load_bundle(src_path)
    if "x" not in payload:
        raise KeyError("Dataset payload missing 'x' tensor")

    stack = payload["x"]
    if not isinstance(stack, torch.Tensor):
        raise TypeError(f"Expected 'x' to be torch.Tensor, received {type(stack)!r}")

    size = stack.shape[0]

    label_names = payload.get("label_names")
    if label_names is None or len(label_names) != size:
        if label_names_fallback is not None and len(label_names_fallback) == size:
            label_names = list(map(str, label_names_fallback))
        else:
            raise KeyError("Dataset payload missing 'label_names' aligned with samples.")
    else:
        label_names = [str(item) for item in label_names]

    raw_labels = payload.get("labels")
    if raw_labels is None:
        label_ids = list(range(size))
    elif isinstance(raw_labels, torch.Tensor):
        label_ids = [int(v) for v in raw_labels.detach().cpu().tolist()]
    else:
        label_ids = [int(v) for v in raw_labels]

    if len(label_ids) != size:
        raise ValueError(f"Label list length ({len(label_ids)}) does not match tensor length ({size}).")

    updated = False
    for group in duplicate_index_groups:
        if not group:
            continue
        canonical_id = min(label_ids[idx] for idx in group)
        for idx in group:
            if label_ids[idx] != canonical_id:
                label_ids[idx] = canonical_id
                updated = True

    labels2label_names: Dict[int, List[str]] = {}
    temp_builder: Dict[int, set] = defaultdict(set)
    for label_id, description in zip(label_ids, label_names):
        temp_builder[int(label_id)].add(str(description))
    for label_id, descriptions in temp_builder.items():
        labels2label_names[label_id] = sorted(descriptions)

    y_tensor = torch.tensor(label_ids, dtype=torch.long, device=stack.device)

    payload["labels"] = label_ids
    payload["labels2label_names"] = labels2label_names
    payload["y"] = y_tensor
    payload.setdefault("label_names", label_names)

    meta = dict(payload.get("meta", {}))
    meta.update(
        {
            "stack_len": size,
            "label_names_count": len(label_names),
            "per_channel_tolerance": PER_CHANNEL_TOLERANCE,
            "mean_tolerance": MEAN_TOLERANCE,
            "label_updates": {
                "groups_processed": len(duplicate_index_groups),
                "labels_changed": int(updated),
            },
        }
    )
    payload["meta"] = meta

    destination_arg = output_path if output_path not in (None, "None", "none") else src_path
    dest_path = Path(destination_arg).expanduser().resolve()

    if dest_path.exists() and dest_path.is_file() and not overwrite:
        return {
            "output_path": str(dest_path),
            "updated": False,
            "groups": len(duplicate_index_groups),
            "skipped_write": True,
        }

    if dest_path.is_dir() or not dest_path.suffix:
        dest_path.mkdir(parents=True, exist_ok=True)
        dataset_io.save_bundle(payload, dest_path)
    else:
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        torch.save(payload, dest_path)

    return {
        "output_path": str(dest_path),
        "updated": updated,
        "groups": len(duplicate_index_groups),
        "skipped_write": False,
    }


def run_duplicate_search(
    bundle: Union[str, Path, None] = None,
    per_tol: float | None = None,
    mean_tol: float | None = None,
) -> List[Dict[str, Any]]:
    """
    Notebook-friendly entry point. Falls back to module-level constants when parameters are omitted.
    Returns a list of summaries where each item contains a canonical label_id, group size, and descriptions.
    """
    if bundle is None:
        if BUNDLE_PATH is None:
            raise ValueError("Bundle path not provided. Set BUNDLE_PATH or pass bundle=...")
        bundle = BUNDLE_PATH

    per_tol = PER_CHANNEL_TOLERANCE if per_tol is None else per_tol
    mean_tol = MEAN_TOLERANCE if mean_tol is None else mean_tol
    stack, label_names, label_ids, labels2label_names = load_dataset(
        bundle, device=COMPARISON_DEVICE
    )
    index_groups = find_partitioned_duplicate_index_groups(
        stack, label_names, per_tol, mean_tol, partition=PARTITION_BY_CUE
    )
    return summarize_duplicate_groups(index_groups, label_ids, label_names, labels2label_names)


def process_duplicates(
    bundle_path: Union[str, Path, None] = BUNDLE_PATH,
    per_tol: float = PER_CHANNEL_TOLERANCE,
    mean_tol: float = MEAN_TOLERANCE,
    output_bundle: Union[str, Path, None] = OUTPUT_BUNDLE_PATH,
    csv_path: Union[str, Path, None] = CSV_OUTPUT_PATH,
    overwrite: bool = OVERWRITE_OUTPUTS,
    partition_by_cue: bool = PARTITION_BY_CUE,
) -> Dict[str, Any]:
    """
    High-level helper: detect duplicates, optionally save a report, and update the dataset bundle.
    Returns a dictionary containing summary info and the update result.
    """
    if bundle_path is None:
        raise ValueError("Bundle path is required. Set BUNDLE_PATH or pass bundle_path=...")
    bundle_path = Path(bundle_path).expanduser().resolve()

    if output_bundle in (None, "None", "none"):
        target_bundle_dir = bundle_path
        output_bundle_arg: Union[str, Path, None] = None
    else:
        target_bundle_dir = Path(output_bundle).expanduser().resolve()
        output_bundle_arg = target_bundle_dir

    stack, label_names, label_ids, labels2label_names = load_dataset(
        bundle_path, device=COMPARISON_DEVICE
    )
    index_groups = find_partitioned_duplicate_index_groups(
        stack, label_names, per_tol, mean_tol, partition=partition_by_cue
    )
    duplicate_label_groups = [
        [label_names[idx] for idx in group]
        for group in index_groups
    ]
    duplicate_label_groups = [
        [label_names[idx] for idx in group]
        for group in index_groups
    ]
    singleton_indices = _singleton_indices(stack.shape[0], index_groups)
    singleton_groups = [[label_names[idx]] for idx in singleton_indices]
    singleton_summaries = _build_singleton_summaries(
        singleton_indices, label_ids, label_names, labels2label_names
    )
    summaries = summarize_duplicate_groups(index_groups, label_ids, label_names, labels2label_names)

    update_result = apply_duplicate_label_updates(
        bundle_path,
        index_groups,
        label_names_fallback=label_names,
        output_path=output_bundle_arg,
        overwrite=overwrite,
    )

    csv_arg = csv_path
    if csv_arg in (None, "None", "none"):
        csv_target = _default_csv_path(target_bundle_dir, per_tol, mean_tol)
    else:
        csv_target = Path(csv_arg).expanduser().resolve()

    ordered_csv_groups = _ordered_groups_by_catalog(
        duplicate_label_groups,
        index_groups,
        singleton_groups,
        singleton_indices,
        label_ids,
        labels2label_names,
    )

    csv_saved, csv_skipped = _write_group_csv(
        csv_target,
        ordered_csv_groups,
        overwrite=overwrite,
    )

    return {
        "summaries": summaries,
        "singleton_summaries": singleton_summaries,
        "singleton_groups": singleton_groups,
        "singleton_count": len(singleton_indices),
        "csv_path": csv_saved,
        "csv_skipped": csv_skipped,
        "update": update_result,
    }



result = process_duplicates()
summaries = result["summaries"]
if not summaries:
    print("No duplicate groups found with the configured tolerances.")
else:
    print(f"Detected {len(summaries)} duplicate group(s):")
    for summary in summaries:
        print(f"  Label {summary['label_id']} (size {summary['size']}):")
        for desc in summary["descriptions"]:
            print(f"    - {desc}")

if result.get("csv_path"):
    if result.get("csv_skipped"):
        print(f"Duplicate CSV already exists at {result['csv_path']}; skipping write.")
    else:
        print(f"Duplicate CSV saved to {result['csv_path']}.")

update = result["update"]
if update.get("skipped_write"):
    print(
        f"Dataset bundle already exists at {update['output_path']}; skipping write (set OVERWRITE_OUTPUTS=True to overwrite)."
    )
elif update["updated"]:
    print(
        f"Updated label IDs written to {update['output_path']} "
        f"({update['groups']} groups processed)."
    )
else:
    print(
        f"No label ID changes were necessary. "
        f"Existing bundle remains at {update['output_path']}."
    )

singleton_count = result.get("singleton_count")
if singleton_count is not None:
    print(f"Singleton samples (non-duplicates): {singleton_count}")
