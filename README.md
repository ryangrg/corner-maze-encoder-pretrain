# Corner Maze Encoder Pretrain

This repository explores how to compress visual observations collected from a rodent-scale corner maze into a compact latent space that can condition downstream reinforcement learning (RL) agents. The raw observations are stereo renderings (left- and right-eye) of the 2s2c behavioral task inside a Fusion360 model of the maze. Each pose in an 11Ã—11 spatial grid produces four views aligned with the cardinal directions, except at corners where diagonal headings (NE, NW, SE, SW) are captured. All renders are flattened into single-channel (B/W) images and paired so that a single tensor contains both eyes.

## Project Goals
- Build preprocessing utilities that organize the stereo renders, flatten them, and stack them into tensors suitable for PyTorch.
- Prototype multiple encoder architectures (CNNs and alternative tensor representations) to learn latent features that uniquely identify maze poses.
- Evaluate the learned encoders by decoding pose identity and by feeding the latent vectors into lightweight RL agents in a MiniGrid-style simulator of the 2s2c task.

## Workflow
1) Start with a base set of image from blairlab-fusion-capture, from this base set of images all views from all poses on the maze can be generated by rotating Cartesian positions and directional values. The base set includes a maze configuration for first exposure session (exp_x_x_xx_), cued trials (trl_n_n_xx_, trl_e_n_xx_, trl_s_n_xx_, trl_w_n_xx_) , non-cued trials (trl_n_x_xx_) and intertrial intervals (iti_w_x_xx, iti_w_x_sw, iti_w_x_nw) - all base configuration prefixes provided in parenthesis, the suffix for labels gives pose _x_y_direction_eye. 
2) Create a pytorch friendly dataset directory using create_dataset.py. The directory includes metadata the tensor image stack (x.npz) and the labels (y.npz). create_dataset.py uses save_bundle() from dataset_io.py module to create a dataset and load_bundle.py across other scripts to load the dataset in pytorch ready format. If building from scratch make sure to unzip image file and create_dataset.py will generate base-images-ds. The tensor created is a stack of 2 x 128 x 28 images where the left and right images are combined for a pose with right eye horizontally mirrored
<u>NOTE:</u> You can use visualize_image_stack.py do go through and view all images in the stack. It will show the tensor stacked image along with the left and right eye vies. Just provided the dataset name and path in DEFAULT_DATASET_DIR. There are more details about this module below.
3) Find duplicate images in the tensor stack. There are many poses across the different maze configurations and some within a configuration that are also the same. group_duplicates.py finds duplicates and updates labels2ladel_name in the metadata for the dataset and will group label_names under the same label that it thinks are duplicates. It will also create a csv file that is easy to inspect where a duplicate group is represented by a row of label_names. Two views that seem identical will not be seen as such using a pixel to pixel comparison of two separately rendered images. This is due to image noise from the rendering. group_duplicates.py has two variable to adjust the allowed tolerance between pixels, PER_CHANNEL_TOLERANCE and MEAN_TOLERANCE. Using values of 52.0 and 1 respectively works well, but one is free to experiment with other settings. group_duplicates.py also has a PARTITION_BY_CUE flag, when true it will group cued-trials separately from the rest of the configurations. Cued-trials have some consistent subtle difference that are PER_CHANNEL_TOLERANCE and MEAN_TOLERANCE can not capture well. When PER_CHANNEL_TOLERANCE is lowered to captures the grouping become too messy.
4) Visual inspection of the grouped images. Once the a grouped set of images is created one can use visualize_image_stack.py to inspect the groupings. If there is a change you want to make to the groups, copy the dataset csv file, give a recognizable name and move it to csv folder in data. There you can fine tune the grouping by creating new row. Again, group_duplicates.py does a good job of grouping the same views such that you will not find the same view across any other groups. The issue that comes up is for a group of images there may be lighting differences on the track and walls. There may also be cases where the monitors appearance change slightly because a barrier interferes with it. In preconfigured datasets labeled acute have been regrouped based on this analyses - details on the process can be found in annotated csv files. datasets with the dull label only regroup based on changes to the monitors within a group.
5) Regroup dataset. Using regroup_dataset.py provided the regrouped csv file base dataset and a new dataset name. The script will regroup the dataset based on the groupings in the provided csv file.
6) Consolidate the grouped tensors in the dataset. Use consolidate_dataset.py to consolidate grouped tensors such that images are reconstructed using the median pixel value for individual the same pixel of all images in the group. Grouped label_names are kept, this is critical, stored in labels2label_names and the csv. The tensor length will now be reduced to the number of groups. This dataset is now ready for classification.
7) Train the dataset.

## Image Clustering Method
Clustering similar views from a pose, by first checking for duplicates allowing for a variance between pixel values.
This method was a first crude pass where too much variance clusters images together that shouldn't be and too
tight of a tolerance leaves out images from clusters. Taking an approach to run a loose variance that clusters
compositionaly similar images but then has a hard time determining if a barrier is in the image or other subtleties.
A view through a barrier renders a monitor slightly more blurry than otherwise. Other subtleties can include odd reflexes
from side wall when there is cue and bright dull spots from barrier reflections.
